\documentclass[12pt, spanish]{article}
\usepackage[spanish]{babel}
\selectlanguage{spanish}
%\usepackage{natbib}
\usepackage{url}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{vmargin}
\usepackage{multirow}
\usepackage{float}
\usepackage{chngpage}

\usepackage{amsfonts}

\usepackage{subcaption}

\usepackage{hyperref}
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
]{doclicense}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

% para codigo
\usepackage{listings}
\usepackage{xcolor}



%% configuración de listings

\definecolor{listing-background}{HTML}{F7F7F7}
\definecolor{listing-rule}{HTML}{B3B2B3}
\definecolor{listing-numbers}{HTML}{B3B2B3}
\definecolor{listing-text-color}{HTML}{000000}
\definecolor{listing-keyword}{HTML}{435489}
\definecolor{listing-identifier}{HTML}{435489}
\definecolor{listing-string}{HTML}{00999A}
\definecolor{listing-comment}{HTML}{8E8E8E}
\definecolor{listing-javadoc-comment}{HTML}{006CA9}

\lstdefinestyle{eisvogel_listing_style}{
  language         = python,
%$if(listings-disable-line-numbers)$
%  xleftmargin      = 0.6em,
%  framexleftmargin = 0.4em,
%$else$
  numbers          = left,
  xleftmargin      = 0em,
 framexleftmargin = 0em,
%$endif$
  backgroundcolor  = \color{listing-background},
  basicstyle       = \color{listing-text-color}\small\ttfamily{}\linespread{1.15}, % print whole listing small
  breaklines       = true,
  frame            = single,
  framesep         = 0.19em,
  rulecolor        = \color{listing-rule},
  frameround       = ffff,
  tabsize          = 4,
  numberstyle      = \color{listing-numbers},
  aboveskip        = 1.0em,
  belowskip        = 0.1em,
  abovecaptionskip = 0em,
  belowcaptionskip = 1.0em,
  keywordstyle     = \color{listing-keyword}\bfseries,
  classoffset      = 0,
  sensitive        = true,
  identifierstyle  = \color{listing-identifier},
  commentstyle     = \color{listing-comment},
  morecomment      = [s][\color{listing-javadoc-comment}]{/**}{*/},
  stringstyle      = \color{listing-string},
  showstringspaces = false,
  escapeinside     = {/*@}{@*/}, % Allow LaTeX inside these special comments
  literate         =
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\'e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\EUR}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
  {…}{{\ldots}}1 {≥}{{>=}}1 {≤}{{<=}}1 {„}{{\glqq}}1 {“}{{\grqq}}1
  {”}{{''}}1
}
\lstset{style=eisvogel_listing_style}


\usepackage[default]{sourcesanspro}

\setmarginsrb{2 cm}{1 cm}{2 cm}{2 cm}{1 cm}{1.5 cm}{1 cm}{1.5 cm}

\title{Práctica 2:\\
Redes Neuronales Convolucionales \hspace{0.05cm} }
\author{Antonio David Villegas Yeguas}
\date{\today}

\renewcommand*\contentsname{hola}

\makeatletter
\let\thetitle\@title
\let\theauthor\@author
\let\thedate\@date
\makeatother

\pagestyle{fancy}
\fancyhf{}
\rhead{\theauthor}
\lhead{\thetitle}
\cfoot{\thepage}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{titlepage}
    \centering
    \vspace*{0.3 cm}
    \includegraphics[scale = 0.50]{ugr.png}\\[0.7 cm]
    %\textsc{\LARGE Universidad de Granada}\\[2.0 cm]
    \textsc{\large 4º CSI 2020/21 - Grupo 2}\\[0.5 cm]
    \textsc{\large Grado en Ingeniería Informática}\\[0.5 cm]
    \rule{\linewidth}{0.2 mm} \\[0.2 cm]
    { \huge \bfseries \thetitle}\\
    \rule{\linewidth}{0.2 mm} \\[1 cm]

    \begin{minipage}{0.4\textwidth}
        \begin{flushleft} \large
            \emph{Autor:}\\
            \theauthor\\
			 \emph{DNI:}\\
            77021623-M
            \end{flushleft}
            \end{minipage}~
            \begin{minipage}{0.4\textwidth}
            \begin{flushright} \large
            \emph{Asignatura: \\
            Visión por Computador}   \\
            \emph{Correo:}\\
            advy99@correo.ugr.es
        \end{flushright}
    \end{minipage}\\[0.5cm]

    {\large \thedate}\\[0.5cm]
    %{\url{https://github.com/advy99/AA/}}
    {\doclicenseThis}

    \vfill

\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{Introducción}

En esta práctica trabajaremos con redes neuronales convolucionales profundas, de cara a obtener experiencia en el diseño y entrenamiento de las mismas utilizando el módulo Keras de Python.

Las redes neuronales convolucionales profundas se componen de operaciones estocásticas, por lo que para obtener los mismos resultados en todos los casos he mantenido la misma semilla en todas las ejecuciones y es la semilla establecida en el código entregado junto a este PDF.

\section{Apartado 1: BaseNet en CIFAR100}

Trabajaremos con el conjunto de datos CIFAR100, que consta de 60.000 imágenes de dimensión 32$\times$32$\times$3 de 100 clases distintas, es decir, 600 imágenes por clase. El conjunto dispone de 50.000 imágenes de entrenamiento y 10.000 imágenes de prueba. Para el desarollo de la práctica solo consideraremos 25 de las 100 clases, por lo tanto el conjunto de entrenamiento tendrá 12500 imágenes y el de prueba 2500. En todos los casos utilizarémos el 10\% del conjunto de entrenamiento como validación.


El modelo inicial que implementaremos será el siguiente:


% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Número de capa} & \textbf{Tipo de capa} & \textbf{\begin{tabular}[c]{@{}c@{}}Tam. kernel \\ (para convoluciones)\end{tabular}} & \textbf{Tamaño de entrada/salida} & \textbf{\begin{tabular}[c]{@{}c@{}}Canales de entrada/salida \\ para convoluciones)\end{tabular}} \\ \hline
1                       & Conv2D                & 5                                                                                    & 32 | 28                           & 3 | 6                                                                                             \\ \hline
2                       & Relu                  & -                                                                                    & 28 | 28                           & -                                                                                                 \\ \hline
3                       & MaxPooling2D          & 2                                                                                    & 28 | 14                           & -                                                                                                 \\ \hline
4                       & Conv2D                & 5                                                                                    & 14 | 10                           & 6 | 16                                                                                            \\ \hline
5                       & Relu                  & -                                                                                    & 10 | 10                           & -                                                                                                 \\ \hline
6                       & MaxPooling2D          & 2                                                                                    & 10 | 5                            & -                                                                                                 \\ \hline
7                       & Linear                & -                                                                                    & 400 | 50                          & -                                                                                                 \\ \hline
8                       & Relu                  & -                                                                                    & 50 | 50                           & -                                                                                                 \\ \hline
9                       & Linear                & -                                                                                    & 50 | 25                           & -                                                                                                 \\ \hline
\end{tabular}%
}
\end{table}


\subsection{Implementación del modelo: Hiperparámetros y función de cada capa}

Para implementar el modelo utilizaremos la clase \texttt{Sequential} del módulo de Python \texttt{keras.models}, que nos permite agrupar una serie de capas de una red que finalmente conformarán un objeto de la clase \texttt{Model}.

En este modelo encontramos distintos tipos de capas:

\begin{itemize}
	\item Conv2D\cite{conv2d} : Capa cuya función es aplicar una convolución 2D a la entrada.
	\item Relu\cite{relu} : Capa que implementa la función de activación ReLu, vista en teoría.
	\item MaxPooling2D\cite{maxpooling2d} : Implementa la operación de max pooling para datos 2D. Reduce la muestra del input en el valor máximo dado por \texttt{pool\_size} en cada eje.
	\item Linear : Implementada utilizando la capa \texttt{Dense}\cite{dense} de Keras, se trata de una capa densa totalmente conectada.
\end{itemize}

Tras esta simple introducción, pasaré a comentar detalladamente los hiperparámetros de cada capa utilizada:

\subsubsection{Capa Conv2D}

En esta capa encontraremos distintos hiperparámetros, principalmente para controlar el tamaño de la máscara, la dimensionalidad de la salida, y otras especificaciones sobre las convoluciones 2D que vimos en la práctica 1. Los más interesantes son:

\begin{itemize}
	\item \texttt{filters}: Dimensionalidad de la salida, es decir, número de filtros de salida en la convolución.
	\item \texttt{kernel\_size}: Tamaño del kernel 2D a aplicar.
	\item \texttt{strides}: Desplazamiento con que aplicar la convolución, por defecto (1,1) ya que se aplicará a toda la entrada.
	\item \texttt{padding}: Relleno al aplicar la convolución, de cara a poder aplicarla a los bordes de la matriz de entrada.
	\item \texttt{activation}: Aplicar una función de activación al final de la operación.

\end{itemize}

En nuestro caso, el valor de \texttt{filters} tendrá el valor de los canales de salida dados en el modelo, \texttt{kernel\_size} tendrá el valor dado en la tabla del modelo, \texttt{strides} utilizaremos el valor por defecto de cara a aplicar la operación a toda la entrada, \texttt{padding} tomará el valor de \texttt{valid} de cara a no aplicar este relleno, y por último, \texttt{activation} no lo utilizaré, ya que he preferido añadir una capa propia de activación. El resultado es equivalente, pero me resulta más cómodo para realizar una clara distinción de que se tratan de operaciones distintas.

\subsubsection{Capa activación ReLu}

Utilizaremos la capa \texttt{Activation} de Keras, que como único parámetro tiene el tipo de activación. En este caso, como nos pide el ejercicio, utilizaremos como función de activación \texttt{relu}. Esta cápa se encargará de decidir si una neurona es activada o no, calculando el peso de esta y teniendo en cuenta el sesgo que podría añadir. El objetivo de este tipo de capas son introducir no linearidad en el modelo.

\subsubsection{Capa MaxPooling2D}

Tiene como objetivo reducir la dimensionalidad de la entrada.

Aunque comparte los parámetros de \texttt{strides} y \texttt{padding} con la capa Conv2D, en nuestro caso el parámetro que nos interesa será \texttt{pool\_size}, este parámetro indicará el factor de reducción de la entrada. De esta forma, como vemos en la tabla del modelo, al aplicar un factor de escala de dos, reduciremos el tamaño de la entrada a la mitad.


\subsubsection{Capa Dense}

Esta capa se trata de una capa de neuronas totalmente conectadas. En esta operación simplemente utilizaremos el parámetro \texttt{units}, con el que indicaremos el número de neuronas de la capa. El tamaño de salida de esta capa coincidirá con el parámetro \texttt{units}.


\subsubsection{Capa Flatten}


Una observación es que vemos como justo antes de aplicar la capa \texttt{Dense} por primera vez en nuestro modelo, pasamos a tener una salida de tamaño cinco, y la entrada a esta capa será de cuatrocientos. Esto es debido a que utilizaremos una capa \texttt{Flatten}\cite{flatten} que reducirá la entrada como un vector 1D, de forma que la capa de tamaño cinco, al tener dieciséis canales, tendrá tamaño cuatrocientos.


\subsubsection{Implementación del modelo}

El modelo por lo tanto se trata de una variable de tipo \texttt{Sequential}, al que con el método \texttt{add} añadiremos las distintas capas con los hiperparámetros dados. Se puede consultar más a fondo en el fichero de Python adjunto a este PDF.


\subsection{Entrenamiento del modelo y valores de accuracy en los conjuntos de validación y test}

Una vez hemos completado nuestro modelo, utilizando un optimizador (en este caso SGD\cite{sgd}), utilizaremos el método \texttt{compile}\cite{compile} para compilar el modelo. En este método utilizaremos tres parámetros:

\begin{enumerate}
	\item \texttt{loss}: Tipo de perdida del modelo, en nuestro caso \texttt{keras.losses.categorical\_crossentropy}, ya que trabajamos con un problema multiclase.
	\item \texttt{optimizer}: Optimizador a utilizar, como he comentado, en nuestro caso \texttt{SGD}.
	\item \texttt{metrics}: Métricas para medir el error del modelo. En nuestro caso \texttt{accuracy}.

\end{enumerate}


Una vez compilado el modelo, podemos entrenarlo con la función \texttt{fit}, que cuenta con distintos parámetros, la mayoría de ellos vistos en Aprendizaje Automático, como el número de épocas, el tamaño del batch a utilizar, y el porcentaje de validación del conjunto de entrenamiento. En nuestro caso, utilizaremos un valor de $0.1$ para el porcentaje de validación, como nos indica el enunciado, así como un tamaño de batch de 32, ya que es el valor por defecto y funciona bien.

Tras esto, la evolución del entrenamiento del modelo es la siguiente:


\begin{figure}[H]
  \centering
      \includegraphics[width=\textwidth]{1-1-1.png}
 		\caption{Perdida en el conjunto de entrenamiento y validación.}
\end{figure}

\begin{figure}[H]
  \centering
      \includegraphics[width=\textwidth]{1-1-2.png}
 		\caption{Valor de precisión en el conjunto de entrenamiento y validación.}
\end{figure}

Observamos como obtenemos un valor de precisión cercano al 42\% en el conjunto de validación, lo que nos lleva a pensar en un resultado mediocre del modelo.

En el conjunto de test conseguimos un valor de precisión similar:

\begin{lstlisting}
Accuracy en el conjunto test: 0.438
\end{lstlisting}

\section{Apartado 2: Mejora del modelo BaseNet}

Hemos visto que el resultado obtenido no es el mejor posible, por lo que en este apartado trataremos de mejorar el modelo. Como se nos comenta en el enunciado, con una buena combinación de capas podemos obtener que el valor de precisión se acerque al 50\%, por este motivo en este apartado aplicaré mejoras en el modelo hasta llegar a ese porcentaje, y las demás mejoras las dejaré para el ejercicio de bonus, que se nos pide seguir mejorando el modelo.


Debido a que las mejoras que he ido probando mejoraban poco a poco el modelo, cada mejora añadida la he mantenido en la siguiente mejora, es decir, por ejemplo, al aplicar normalización, en la mejora de aumento de datos cuenta con el aumento de datos y con la normalización.

\subsection{Normalización de datos}

De cara a obtener un entrenamiento más sencillo y sólido, he utilizado la normalización de datos. Para aplicar esta normalización he utilizado la clase \texttt{ImageDataGenerator}\cite{imagedatagenerator}



\subsection{Aumento de datos}



\subsection{Normalización con BatchNormalization}


\subsection{Regularización con Dropout}







\section{Apartado 3: Transferencia de modelos y ajuste fino con ResNet50 para la base de datos Caltech-UCSD}

\section{Bonus: Propuestas de mejora de BaseNet}


\newpage

\section{Referencias, material y documentación usada}


\begin{thebibliography}{9}

\bibitem{sequential}
	Clase Sequential - Documentación de Keras

	\url{https://keras.io/api/models/sequential/}


\bibitem{model}

	Clase Model - Documentación de Keras

	\url{https://keras.io/api/models/model/}

\bibitem{conv2d}

	Capa Conv2D - Documentación de Keras

	\url{https://keras.io/api/layers/convolution_layers/convolution2d/}


\bibitem{activation}

	Capa Activation - Documentación de Keras

	\url{https://keras.io/api/layers/core_layers/activation/}

\bibitem{relu}

	Capa ReLu - Documentación de Keras

	\url{https://keras.io/api/layers/activation_layers/relu/}

\bibitem{maxpooling2d}

	Capa MaxPooling2D - Documentación de Keras

	\url{https://keras.io/api/layers/pooling_layers/max_pooling2d/}

\bibitem{dense}

	Capa Dense - Documentación de Keras

	\url{https://keras.io/api/layers/activations/}

\bibitem{flatten}

	Capa Flatten - Documentación de Keras

	\url{https://keras.io/api/layers/reshaping_layers/flatten/}

\bibitem{sgd}

	Optimizador SGD - Documentación de Keras

	\url{https://keras.io/api/optimizers/sgd/}

\bibitem{imagedatagenerator}

	Clase ImageDataGenrator - Documentación de Keras

	\url{https://keras.io/api/preprocessing/image/#imagedatagenerator-class}

\end{thebibliography}

\end{document}
